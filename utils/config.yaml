models:
  bert:
    layer: 'bert.encoder.layer'
    ln1: 'attention.output.LayerNorm'
    ln2: 'output.LayerNorm'
    values: 'attention.self.value'
    dense: 'attention.output.dense'
    pre_layer_norm: 'False'
    ln_before_residual: 'False'
    model_type: 'mlm'
  roberta:
    layer: 'roberta.encoder.layer'
    ln1: 'attention.output.LayerNorm'
    ln2: 'output.LayerNorm'
    values: 'attention.self.value'
    dense: 'attention.output.dense'
    pre_layer_norm: 'False'
    ln_before_residual: 'False'
    model_type: 'mlm'
  electra:
    layer: 'electra.encoder.layer'
    ln1: 'attention.output.LayerNorm'
    ln2: 'output.LayerNorm'
    values: 'attention.self.value'
    dense: 'attention.output.dense'
    pre_layer_norm: 'False'
    ln_before_residual: 'False'
    model_type: 'mlm'
  qwen2:
    layer: 'model.layers'
    ln1: 'input_layernorm'
    ln2: 'post_attention_layernorm'
    values: 'self_attn.v_proj'
    dense: 'self_attn.o_proj'
    pre_layer_norm: 'True'
    ln_before_residual: 'False'
    model_type: 'clm'
  gemma2:
    layer: 'model.layers'
    ln1: 'post_attention_layernorm'
    ln2: 'pre_feedforward_layernorm'
    values: 'self_attn.v_proj'
    dense: 'self_attn.o_proj'
    pre_layer_norm: 'True'
    ln_before_residual: 'True'
    model_type: 'clm'
  llama:
    layer: 'model.layers'
    ln1: 'input_layernorm'
    ln2: 'post_attention_layernorm'
    values: 'self_attn.v_proj'
    dense: 'self_attn.o_proj'
    pre_layer_norm: 'True'
    ln_before_residual: 'False'
    model_type: 'clm'
  distilbert:
    layer: 'distilbert.transformer.layer'
    ln1: 'sa_layer_norm'
    ln2: 'output_layer_norm'
    values: 'attention.v_lin'
    dense: 'attention.out_lin'
    pre_layer_norm: 'False'
    ln_before_residual: 'False'
    model_type: 'mlm'